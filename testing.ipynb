{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de467f8d-cb91-4608-9b81-1ad21482af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e90df3-3007-4540-91e2-7f76d548de7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efb974c-043e-45ad-a34f-7e7a5de78da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Groq:  ········\n"
     ]
    }
   ],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# # %pip install -U langchain-groq\n",
    "\n",
    "# if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#   os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64212aad-b9d5-40ce-adc9-f23023e7915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\") or not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    raise EnvironmentError(\"Missing API keys!\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a134220-1cbd-4389-8045-948fa6dfc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35979728-e25a-4b3f-8ebe-6cad7dbb11e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chengmao\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-huggingface\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113d3a98-96e0-4780-b350-4f14840b0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from networkx.algorithms.community import modularity_max\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f594b0e-932f-467b-8e13-1a58efdbd456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk metadata: {'producer': 'Skia/PDF m134', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'creationdate': '2025-03-28T06:46:56+00:00', 'title': 'HDB | Application for an HDB Flat Eligibility (HFE) Letter', 'moddate': '2025-03-28T06:46:56+00:00', 'source': 'D:/LECTURE/Y4S2/DSA4265/DSA4265/HDB_docs\\\\HDB_Application_for_an_HDB_Flat_Eligibility_(HFE)_Letter.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load NLP model and sentence transformer model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Step 1: Load Documents with Metadata Preservation\n",
    "\n",
    "folder_path = \"D:/LECTURE/Y4S2/DSA4265/DSA4265/HDB_docs\"  # put your own file path to the HDB docs\n",
    "all_docs = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Add source metadata to all pages\n",
    "        for doc in docs:\n",
    "            doc.metadata['source'] = file_path  # Ensure source is preserved\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "# Step 2: Text Chunking with Metadata Inheritance\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,  # Reduced from 300\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"(?<=\\\\. )\", \" \"]  # Better sentence-aware splits\n",
    ")\n",
    "all_splits = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Verify metadata in splits\n",
    "print(\"Sample chunk metadata:\", all_splits[0].metadata)\n",
    "\n",
    "# Step 3: Entity Extraction with Coreference Resolution\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"ORG\", \"GPE\", \"PERSON\", \"EVENT\", \"PRODUCT\"}:\n",
    "            # Normalize entity text and handle coreferences\n",
    "            clean_ent = ent.text.strip().replace('\\n', ' ')\n",
    "            entities[clean_ent] = ent.label_\n",
    "    return entities\n",
    "\n",
    "# Step 4: Relationship Extraction with Context Awareness\n",
    "def extract_relationships(chunks):\n",
    "    relations = []\n",
    "    for chunk in chunks:\n",
    "        entities = list(extract_entities(chunk.page_content).keys())\n",
    "        # Create bidirectional relationships with context\n",
    "        for i in range(len(entities)):\n",
    "            for j in range(i+1, len(entities)):\n",
    "                relations.append((entities[i], \"related_to\", entities[j]))\n",
    "                relations.append((entities[j], \"related_to\", entities[i]))  # Bidirectional\n",
    "    return relations\n",
    "\n",
    "# Step 5: Knowledge Graph Construction with Full Metadata\n",
    "knowledge_graph = nx.DiGraph()\n",
    "entity_mapping = {}\n",
    "\n",
    "for chunk in all_splits:\n",
    "\n",
    "    # Preserve metadata with fallbacks\n",
    "    metadata = chunk.metadata.copy()\n",
    "    metadata.setdefault('source', 'Unknown')\n",
    "    metadata.setdefault('page', 0)\n",
    "\n",
    "    # Document node with full metadata\n",
    "    doc_embedding = sentence_model.encode(chunk.page_content)\n",
    "    doc_node_id = f\"doc_{len(knowledge_graph.nodes)}\"\n",
    "    knowledge_graph.add_node(\n",
    "        doc_node_id,\n",
    "        type=\"document\",\n",
    "        embedding=doc_embedding,\n",
    "        content=chunk.page_content,\n",
    "        metadata=metadata,  # ✅ Store all metadata\n",
    "        label=f\"Document {doc_node_id}\"\n",
    "    )\n",
    "\n",
    "    # Entity handling with normalization\n",
    "    entities = extract_entities(chunk.page_content)\n",
    "    for entity, label in entities.items():\n",
    "        clean_entity = entity.strip().replace('\\n', ' ')\n",
    "        if clean_entity not in entity_mapping:\n",
    "            knowledge_graph.add_node(\n",
    "                clean_entity,\n",
    "                label=label,\n",
    "                type=\"entity\"\n",
    "            )\n",
    "            entity_mapping[clean_entity] = label\n",
    "\n",
    "        # Connect document to entity\n",
    "        knowledge_graph.add_edge(\n",
    "            doc_node_id, clean_entity,\n",
    "            relation=\"mentions\",\n",
    "            weight=1.0\n",
    "        )\n",
    "\n",
    "    # Add entity relationships\n",
    "    for source, rel, target in extract_relationships([chunk]):\n",
    "        clean_source = source.strip().replace('\\n', ' ')\n",
    "        clean_target = target.strip().replace('\\n', ' ')\n",
    "        if clean_source in entity_mapping and clean_target in entity_mapping:\n",
    "            knowledge_graph.add_edge(\n",
    "                clean_source, clean_target,\n",
    "                relation=rel,\n",
    "                weight=1.0\n",
    "            )\n",
    "\n",
    "# Step 6: Community Detection with Enhanced Visualization\n",
    "def detect_communities():\n",
    "    return list(nx.algorithms.community.louvain_communities(knowledge_graph))\n",
    "\n",
    "def visualize_graph():\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    pos = nx.spring_layout(knowledge_graph, k=0.2, seed=42)\n",
    "\n",
    "    # Node styling\n",
    "    node_colors = []\n",
    "    for node in knowledge_graph.nodes():\n",
    "        if 'document' in knowledge_graph.nodes[node].get('type', ''):\n",
    "            node_colors.append('lightgreen')\n",
    "        else:\n",
    "            node_colors.append('skyblue')\n",
    "\n",
    "    nx.draw_networkx_nodes(knowledge_graph, pos, node_size=800, node_color=node_colors)\n",
    "    nx.draw_networkx_edges(knowledge_graph, pos, alpha=0.3, width=1.5)\n",
    "\n",
    "    # Label formatting\n",
    "    labels = {n: d.get('label', n) for n, d in knowledge_graph.nodes(data=True)}\n",
    "    nx.draw_networkx_labels(knowledge_graph, pos, labels, font_size=9)\n",
    "\n",
    "    plt.title(\"Knowledge Graph with Document Metadata and Entity Relationships\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# if len(knowledge_graph.nodes) > 0:\n",
    "#     visualize_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a024e7-8baf-468b-a473-4c8d6098faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---- LLM and sentence model (must be defined beforehand) ----\n",
    "# llm = ChatOpenAI()\n",
    "# sentence_model = SentenceTransformer(...)\n",
    "# knowledge_graph = your NetworkX graph with embedded document nodes\n",
    "\n",
    "# ---- Shared memory structure ----\n",
    "user_profile = {\n",
    "    \"age\": None,\n",
    "    \"income\": None,\n",
    "    \"flat_type\": None,\n",
    "    \"relationship_status\": None,\n",
    "}\n",
    "\n",
    "# ---- LangGraph-compatible State ----\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    hypothetical_doc: Optional[str]\n",
    "    context: List[Tuple[Document, float]]\n",
    "    answer: Optional[str]\n",
    "    messages: List[str]\n",
    "\n",
    "# ---- User profile utilities ----\n",
    "def extract_age(query: str):\n",
    "    match = re.search(r'\\b(?:i am|i’m|im)?\\s*(\\d{2})\\s*(?:years old|y/o|yo|yrs)?\\b', query.lower())\n",
    "    return int(match.group(1)) if match else None\n",
    "    \n",
    "def extract_income(query: str):\n",
    "    match = re.search(r'\\$?\\s?(\\d{3,5})', query)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_relationship(query: str):\n",
    "    q = query.lower()\n",
    "    if any(w in q for w in [\"fiance\", \"fiancée\"]): return \"fiance\"\n",
    "    if any(w in q for w in [\"married\", \"spouse\", \"wife\", \"husband\"]): return \"married\"\n",
    "    if \"divorced\" in q: return \"divorced\"\n",
    "    if \"widowed\" in q or \"orphan\" in q: return \"widowed\"\n",
    "    if \"single\" in q: return \"single\"\n",
    "    return None\n",
    "\n",
    "def extract_flat_type(query: str):\n",
    "    q = query.lower()\n",
    "    \n",
    "    # Handle both / open-ended cases\n",
    "    if any(word in q for word in [\"both\", \"not sure\", \"unsure\", \"either\", \"any\"]):\n",
    "        return \"both\"\n",
    "    \n",
    "    if \"bto\" in q:\n",
    "        return \"bto\"\n",
    "    if \"resale\" in q:\n",
    "        return \"resale\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def update_user_profile(query: str):\n",
    "    user_profile[\"age\"] = extract_age(query) or user_profile[\"age\"]\n",
    "    user_profile[\"income\"] = extract_income(query) or user_profile[\"income\"]\n",
    "    user_profile[\"relationship_status\"] = extract_relationship(query) or user_profile[\"relationship_status\"]\n",
    "    user_profile[\"flat_type\"] = extract_flat_type(query) or user_profile[\"flat_type\"]\n",
    "\n",
    "def ask_missing_fields():\n",
    "    prompts = {\n",
    "        \"age\": \"🔎 What is your age? \",\n",
    "        \"income\": \"💰 What is your monthly income? \",\n",
    "        \"relationship_status\": \"❤️ What is your relationship status? \",\n",
    "        \"flat_type\": \"🏠 Are you interested in a BTO or resale flat? \",\n",
    "    }\n",
    "    \n",
    "    for key, prompt in prompts.items():\n",
    "        if not user_profile[key]:\n",
    "            response = input(prompt)\n",
    "            update_user_profile(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59cec223-414c-4daa-b58b-2ccea783e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ No saved profile found. Using defaults.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chengmao\\AppData\\Local\\Temp\\ipykernel_29104\\3514564380.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. Short-term memory (conversation tracking)\n",
    "chat_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 2. Long-term memory persistence (user profile)\n",
    "PROFILE_PATH = \"user_profile.json\"\n",
    "\n",
    "def save_user_profile():\n",
    "    with open(PROFILE_PATH, \"w\") as f:\n",
    "        json.dump(user_profile, f)\n",
    "\n",
    "def load_user_profile():\n",
    "    global user_profile\n",
    "    if os.path.exists(PROFILE_PATH):\n",
    "        with open(PROFILE_PATH, \"r\") as f:\n",
    "            user_profile = json.load(f)\n",
    "        print(\"✅ User profile loaded.\")\n",
    "    else:\n",
    "        print(\"ℹ️ No saved profile found. Using defaults.\")\n",
    "\n",
    "# Load profile at startup\n",
    "load_user_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0c1627-52ad-41e6-a9e8-f1d3cb77beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- LangGraph nodes ----\n",
    "def generate_hypothetical_node(state: State) -> State:\n",
    "    profile = user_profile\n",
    "    flat = profile.get(\"flat_type\")\n",
    "\n",
    "    # 🔀 Flat type context handling\n",
    "    if flat == \"bto\":\n",
    "        flat_context = \"The user is interested in a BTO flat.\"\n",
    "    elif flat == \"resale\":\n",
    "        flat_context = \"The user is interested in a resale flat.\"\n",
    "    elif flat == \"both\":\n",
    "        flat_context = (\n",
    "            \"The user is open to both BTO and resale flats. \"\n",
    "            \"If policies differ, simulate an answer that includes both perspectives.\"\n",
    "        )\n",
    "    else:\n",
    "        flat_context = (\n",
    "            \"The user has not specified flat type. \"\n",
    "            \"Please simulate an answer that covers both BTO and resale paths if relevant.\"\n",
    "        )\n",
    "\n",
    "    # 📋 Full profile summary\n",
    "    profile_summary = f\"\"\"User Profile:\n",
    "- Age: {profile.get('age')}\n",
    "- Income: {profile.get('income')}\n",
    "- Relationship: {profile.get('relationship_status')}\n",
    "- Flat Type: {flat or 'unspecified'}\n",
    "\n",
    "{flat_context}\n",
    "\"\"\"\n",
    "\n",
    "    # 🧠 HyDE generation prompt\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": f\"Generate a hypothetical answer as if it came from a government housing policy document. Use the user's profile below:\\n\\n{profile_summary}\"},\n",
    "        {\"role\": \"user\", \"content\": state[\"question\"]}\n",
    "    ])\n",
    "    \n",
    "    state[\"hypothetical_doc\"] = response.content\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_node(state: State) -> State:\n",
    "    hyde_embedding = sentence_model.encode(state[\"hypothetical_doc\"])\n",
    "    results = []\n",
    "    seen_hashes = set()\n",
    "    for node_id, node_data in knowledge_graph.nodes(data=True):\n",
    "        if node_data.get('type') != 'document': continue\n",
    "        doc_embed = node_data.get('embedding')\n",
    "        if doc_embed is None: continue\n",
    "        score = cosine_similarity(\n",
    "            hyde_embedding.reshape(1, -1), \n",
    "            doc_embed.reshape(1, -1)\n",
    "        )[0][0]\n",
    "        doc = Document(\n",
    "            page_content=node_data.get('content', ''),\n",
    "            metadata=node_data.get('metadata', {})\n",
    "        )\n",
    "        hash_ = hashlib.md5(doc.page_content.encode()).hexdigest()\n",
    "        if hash_ not in seen_hashes:\n",
    "            results.append((doc, score))\n",
    "            seen_hashes.add(hash_)\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    state[\"context\"] = results[:3]\n",
    "    return state\n",
    "\n",
    "def generate_node(state: State) -> State:\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc, _ in state[\"context\"]])\n",
    "    # history = \"\\n\".join(state.get(\"messages\", [])[-5:])\n",
    "    history = chat_memory.load_memory_variables({}).get(\"chat_history\", \"\")\n",
    "    \n",
    "    # ✅ Add profile logic\n",
    "    profile = user_profile\n",
    "    flat = profile.get(\"flat_type\")\n",
    "\n",
    "    # 🔀 Flat type context handling\n",
    "    if flat == \"bto\":\n",
    "        flat_context = \"The user is interested in a BTO flat.\"\n",
    "    elif flat == \"resale\":\n",
    "        flat_context = \"The user is interested in a resale flat.\"\n",
    "    elif flat == \"both\":\n",
    "        flat_context = (\n",
    "            \"The user is open to both BTO and resale flats. \"\n",
    "            \"If policies differ, explain both options side by side.\"\n",
    "        )\n",
    "    else:\n",
    "        flat_context = (\n",
    "            \"The user has not specified BTO or resale. \"\n",
    "            \"If the answer depends on flat type, explain both options clearly.\"\n",
    "        )\n",
    "\n",
    "    # 📋 Full profile summary to include in prompt\n",
    "    profile_summary = f\"\"\"\n",
    "User Profile:\n",
    "- Age: {profile.get('age')}\n",
    "- Income: {profile.get('income')}\n",
    "- Relationship: {profile.get('relationship_status')}\n",
    "- Flat Type: {flat or 'unspecified'}\n",
    "\n",
    "{flat_context}\n",
    "\"\"\"\n",
    "\n",
    "    # 🧠 Prompt messages\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"You are an HDB assistant. Use the following user profile and retrieved documents to answer accurately.\n",
    "\n",
    "{profile_summary}\n",
    "\n",
    "📚 Retrieved Context:\n",
    "{context_text}\n",
    "\n",
    "🧠 Chat History:\n",
    "{history if history else \"No prior chat history.\"}\n",
    "\"\"\" },\n",
    "        {\"role\": \"user\", \"content\": state['question']}\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Track conversation turn in memory\n",
    "    chat_memory.save_context({\"input\": state[\"question\"]}, {\"output\": response.content})\n",
    "\n",
    "    state[\"answer\"] = response.content\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def fact_check_answer(answer: str, docs: List, threshold: float = 0.6) -> bool:\n",
    "    answer_sentences = [s.strip() for s in answer.split('.') if len(s.strip()) > 10]\n",
    "\n",
    "    # Gracefully handle both Document and (Document, score)\n",
    "    cleaned_docs = [doc[0] if isinstance(doc, tuple) else doc for doc in docs]\n",
    "\n",
    "    for sentence in answer_sentences:\n",
    "        for doc in cleaned_docs:\n",
    "            sim = SequenceMatcher(None, sentence.lower(), doc.page_content.lower()).ratio()\n",
    "            if sim > threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def fact_check_node(state: State) -> State:\n",
    "    answer = state.get(\"answer\", \"\")\n",
    "    docs = [doc[0] if isinstance(doc, tuple) else doc for doc in state.get(\"context\", [])]\n",
    "\n",
    "    if not fact_check_answer(answer, docs):\n",
    "        print(\"⚠️ Fact-check failed. Switching to safe RAG answer.\")\n",
    "\n",
    "        context_text = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        # ✅ Reuse full user profile in fallback\n",
    "        profile = user_profile\n",
    "        flat = profile.get(\"flat_type\")\n",
    "        if flat == \"bto\":\n",
    "            flat_context = \"The user is interested in a BTO flat.\"\n",
    "        elif flat == \"resale\":\n",
    "            flat_context = \"The user is interested in a resale flat.\"\n",
    "        elif flat == \"both\":\n",
    "            flat_context = (\n",
    "                \"The user is open to both BTO and resale options. \"\n",
    "                \"If policies differ, explain both clearly.\"\n",
    "            )\n",
    "        else:\n",
    "            flat_context = (\n",
    "                \"The user has not specified flat type. \"\n",
    "                \"If relevant, explain both BTO and resale differences.\"\n",
    "            )\n",
    "\n",
    "        profile_summary = f\"\"\"\n",
    "User Profile:\n",
    "- Age: {profile.get('age')}\n",
    "- Income: {profile.get('income')}\n",
    "- Relationship: {profile.get('relationship_status')}\n",
    "- Flat Type: {flat or 'unspecified'}\n",
    "\n",
    "{flat_context}\n",
    "\"\"\"\n",
    "\n",
    "        fallback_prompt = [\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"You are an HDB assistant. Always answer based on the user profile and retrieved documents.\n",
    "\n",
    "{profile_summary}\n",
    "\n",
    "📚 Retrieved Context:\n",
    "{context_text}\n",
    "\"\"\" },\n",
    "            {\"role\": \"user\", \"content\": state[\"question\"]}\n",
    "        ]\n",
    "\n",
    "        fallback = llm.invoke(fallback_prompt)\n",
    "        state[\"answer\"] = fallback.content\n",
    "    else:\n",
    "        print(\"✅ Answer passed fact-check.\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204ecd56-01c0-49cd-9dbd-f3fc200b7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Build LangGraph ----\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"generate_hypothesis\", generate_hypothetical_node)\n",
    "workflow.add_node(\"retrieve\", retrieve_node)\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "workflow.add_node(\"fact_check\", fact_check_node)\n",
    "\n",
    "workflow.set_entry_point(\"generate_hypothesis\")\n",
    "workflow.add_edge(\"generate_hypothesis\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"fact_check\")\n",
    "workflow.set_finish_point(\"fact_check\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90820556-085d-427b-aae2-81af424ba65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# A dictionary to store user profiles keyed by serial\n",
    "user_memory_store = {}\n",
    "\n",
    "# Generate unique user ID (e.g., 6-char alphanumeric code)\n",
    "def generate_user_id(length=6):\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))\n",
    "    \n",
    "def check_existing_user():\n",
    "    print(\"🔁 Welcome back! If you're a returning user, enter your serial code. Otherwise, press Enter to start fresh.\")\n",
    "    user_input = input(\"🔑 Serial code: \").strip().upper()\n",
    "\n",
    "    if user_input and user_input in user_memory_store:\n",
    "        print(f\"✅ Found your profile with ID {user_input}. Welcome back!\")\n",
    "        user_profile.update(user_memory_store[user_input])\n",
    "        return user_input\n",
    "    else:\n",
    "        new_id = generate_user_id()\n",
    "        print(f\"🆕 New session started. Your serial code is: {new_id}\")\n",
    "        print(\"📌 Save this code to continue later with the same profile. And what do you want to ask?\")\n",
    "        return new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61acdd8-7233-4aa2-8352-94f56c6b0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Interactive chatbot ----\n",
    "chat_history = []\n",
    "\n",
    "def interactive_chatbot():\n",
    "    print(\"👋 Hello! I'm your HDB eligibility assistant.\")\n",
    "    session_id = check_existing_user()  # 🔑 New or returning?\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\n🧑 You: \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            # 💾 Save user profile\n",
    "            user_memory_store[session_id] = user_profile.copy()\n",
    "            print(f\"📦 Profile saved under ID: {session_id}\")\n",
    "            print(\"👋 Goodbye! Use your serial next time to continue.\")\n",
    "            break\n",
    "\n",
    "        update_user_profile(query)\n",
    "        ask_missing_fields()\n",
    "\n",
    "        chat_history.append(f\"User: {query}\")\n",
    "\n",
    "        result = graph.invoke({\n",
    "            \"question\": query,\n",
    "            \"hypothetical_doc\": None,\n",
    "            \"context\": [],\n",
    "            \"answer\": None,\n",
    "            \"messages\": chat_history\n",
    "        })\n",
    "\n",
    "        answer = result[\"answer\"]\n",
    "        chat_history.append(f\"Assistant: {answer}\")\n",
    "\n",
    "        print(\"\\n🤖\", answer)\n",
    "        print(f\"\\n🧠 Current user profile (ID: {session_id}):\", user_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f92e4a-ccca-43be-be50-fe33f02c2285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Hello! I'm your HDB eligibility assistant.\n",
      "🔁 Welcome back! If you're a returning user, enter your serial code. Otherwise, press Enter to start fresh.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🔑 Serial code:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🆕 New session started. Your serial code is: PH1AAZ\n",
      "📌 Save this code to continue later with the same profile. And what do you want to ask?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🧑 You:  i want to buy a new hdb FLAT , is there any requirements?\n",
      "🔎 What is your age?  27\n",
      "💰 What is your monthly income?  3400\n",
      "❤️ What is your relationship status?  single\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Fact-check failed. Switching to safe RAG answer.\n",
      "\n",
      "🤖 Congratulations on considering buying a new HDB flat!\n",
      "\n",
      "As a 27-year-old single individual with an income of $3400, I'd like to guide you through the eligibility requirements for buying a new HDB flat.\n",
      "\n",
      "Based on the retrieved context, here are the key requirements:\n",
      "\n",
      "1. **Age**: You meet the age requirement, as you're below 35 years old.\n",
      "2. **Income**: Your average gross monthly household income must not exceed $4,500. Since you're single, your income is the only factor to consider, and you comfortably meet this requirement.\n",
      "3. **Employment**: You must have worked continuously for at least 12 months, 2 months before the HFE letter application, and be working at the time of the HFE letter application. As you're single, this requirement likely applies to you.\n",
      "4. **Remaining Lease of Flat**: The flat you're interested in must have a remaining lease of more than 20 years.\n",
      "\n",
      "Additionally, as a single first-time applicant, you're eligible to apply for a BTO flat or a resale flat under the HDB's eligibility conditions.\n",
      "\n",
      "However, please note that there are different eligibility conditions for buying a BTO flat compared to a resale flat. Here's a summary:\n",
      "\n",
      "**BTO Flat Eligibility Conditions**:\n",
      "\n",
      "* First-time applicants aged 21 and above who are buying a flat on their own or with other first-time single citizens.\n",
      "* You meet the employment and income requirements mentioned above.\n",
      "\n",
      "**Resale Flat Eligibility Conditions**:\n",
      "\n",
      "* First-time applicants aged 21 and above who are buying a resale flat on their own, with other first-time single citizens, or with their parents.\n",
      "* You meet the employment and income requirements mentioned above.\n",
      "\n",
      "As you're open to both BTO and resale options, I recommend checking the HDB's website for the latest eligibility conditions and requirements. You may also want to consult with an HDB assistant or a real estate agent for personalized guidance.\n",
      "\n",
      "Remember to carefully review the eligibility conditions and requirements before applying for a BTO or resale flat. Good luck with your flat hunt!\n",
      "\n",
      "🧠 Current user profile (ID: PH1AAZ): {'age': 27, 'income': 3400, 'flat_type': 'both', 'relationship_status': 'single'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m interactive_chatbot()\n",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36minteractive_chatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m session_id \u001b[38;5;241m=\u001b[39m check_existing_user()  \u001b[38;5;66;03m# 🔑 New or returning?\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🧑 You: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# 💾 Save user profile\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         user_memory_store[session_id] \u001b[38;5;241m=\u001b[39m user_profile\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "interactive_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ca219-cde7-4e78-8930-5965c568cd71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
